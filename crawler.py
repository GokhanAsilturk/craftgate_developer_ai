# crawler.py
import json
import logging
import os
import re
import time
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

from config import Config

logger = logging.getLogger(__name__)


def normalize_url(url: str) -> str:
    """URL'yi normalleÅŸtir: query ve fragmentleri kaldÄ±r."""
    parsed = urlparse(url)
    return parsed.scheme + "://" + parsed.netloc + parsed.path


def normalize_text(text: str) -> str:
    """Metni normalleÅŸtir: fazla boÅŸluklarÄ± kaldÄ±r, bÃ¼yÃ¼k-kÃ¼Ã§Ã¼k harf farkÄ±nÄ± ortadan kaldÄ±r."""
    text = re.sub(r'\s+', ' ', text).strip()  # Fazla boÅŸluklarÄ± kaldÄ±r
    text = text.lower()  # BÃ¼yÃ¼k-kÃ¼Ã§Ã¼k harf farkÄ±nÄ± kaldÄ±r
    return text


CACHE_FILE = "craftgate_crawled_contents.json"
CACHE_TTL = 24 * 3600  # 24 saat


def load_cached() -> Optional[List[Dict[str, str]]]:
    if not os.path.exists(CACHE_FILE):
        return None
    if time.time() - os.path.getmtime(CACHE_FILE) > CACHE_TTL:
        return None
    with open(CACHE_FILE, encoding="utf-8") as f:
        logger.info("ğŸ—„ï¸ Cache yÃ¼klendi, tarama atlandÄ±.")
        return json.load(f)


def save_cache(chunks: List[Dict[str, str]]):
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)
    logger.info("ğŸ—„ï¸ Yeni tarama tamamlandÄ±, cache'e kaydedildi.")


def crawl_website_with_cache(base_url: str, force_refresh: bool = False) -> List[Dict[str, str]]:
    if not force_refresh:
        cached = load_cached()
        if cached is not None:
            return cached

    # force_refresh=True ya da cache yaÅŸlÄ±ysa buraya dÃ¼ÅŸer
    chunks = crawl_website(base_url)
    save_cache(chunks)

    # Debug iÃ§in iÃ§erikleri bir txt dosyasÄ±na kaydedelim
    # with open("crawled_contents.txt", "w", encoding="utf-8") as f:
    #     for i, chunk in enumerate(chunks):
    #         f.write(f"URL: {chunk['url']}\n")
    #         f.write(f"BaÅŸlÄ±k: {chunk.get('title', '')}\n")
    #         f.write(f"Ä°Ã§erik: {chunk['text'][:500]}...\n\n")

    return chunks


def crawl_website(base_url: str) -> List[Dict[str, str]]:
    """
    Web sitesi sayfalarÄ±nÄ± tarar ve iÃ§erik+HTML ÅŸeklinde dÃ¶ndÃ¼rÃ¼r.
    """
    visited = set()
    to_visit = [base_url]
    all_pages = []
    seen_urls = set()

    while to_visit:
        url = to_visit.pop(0)
        normalized_url = normalize_url(url)

        if normalized_url in visited or normalized_url in seen_urls:
            continue

        seen_urls.add(normalized_url)

        if '/en' in urlparse(url).path:
            continue

        try:
            logger.info(f"Sayfa taranÄ±yor: {url}")
            response = requests.get(url, headers={"User-Agent": Config.USER_AGENT}, timeout=30)
            response.raise_for_status()

            content_type = response.headers.get('Content-Type', '')
            if 'text/html' not in content_type:
                continue

            html_content = response.text
            soup = BeautifulSoup(html_content, 'html.parser')

            # Dil kontrolÃ¼ yap
            html_tag = soup.find('html')
            if html_tag and html_tag.get('lang') != 'tr':
                continue

            # Ä°Ã§eriÄŸi Ã§Ä±kart
            title = soup.title.text.strip() if soup.title else ""

            # Ana iÃ§eriÄŸi Ã§Ä±kar
            main_content = extract_main_content(html_content)

            # HTML'i temizle
            simplified_html = simplify_html(html_content)

            # SayfayÄ± sakla
            page = {
                'url': url,
                'title': title,
                'text': main_content,  # VektÃ¶r aramalar iÃ§in metin iÃ§erik
                'html': simplified_html,  # LLM iÃ§in HTML iÃ§erik
                'content_index': len(all_pages)  # Ä°ndeks bilgisi
            }

            all_pages.append(page)
            print(f"Ã‡ekilen iÃ§erik: {title[:50]}... (URL: {url})")

            # Linkleri ekle
            for link in soup.find_all('a', href=True):
                href = link['href']
                absolute_url = urljoin(base_url, href)
                parsed_link = urlparse(absolute_url)
                if parsed_link.netloc == urlparse(base_url).netloc and '/en' not in parsed_link.path:
                    to_visit.append(absolute_url)

            visited.add(normalized_url)

        except Exception as e:
            logger.error(f"Tarama hatasÄ± {url}: {e}")

    logger.info(f"Toplam {len(all_pages)} sayfa Ã§ekildi.")
    return all_pages


def extract_main_content(html_content: str):
    """
    HTML iÃ§eriÄŸinden ana metni metin olarak Ã§Ä±karÄ±r.
    """
    # Gelen HTML string'inden BeautifulSoup objesi oluÅŸtur
    soup = BeautifulSoup(html_content, 'html.parser')

    # Ana iÃ§erik containerlarÄ±
    main_selectors = [
        'main', 'article', 'div.content', 'div.main-content',
        'div#content', 'section.content', 'div[role="main"]'
    ]

    main_content_text = ""  # Ana metni tutacak deÄŸiÅŸken

    for selector in main_selectors:
        main_content_element = soup.select_one(selector)
        if main_content_element:
            # Sadece get_text() Ã§aÄŸÄ±rÄ±n, sonra temizleyin
            main_content_text = main_content_element.get_text()
            break  # Ä°lk bulunan ana iÃ§eriÄŸi alÄ±p dÃ¶ngÃ¼yÃ¼ kÄ±r

    # EÄŸer ana iÃ§erik selectorlarÄ± ile bir ÅŸey bulunamazsa fallback'e geÃ§
    if not main_content_text:
        content_divs = [(div, len(div.get_text())) for div in soup.find_all('div')
                        if len(div.get_text().strip()) > 100]

        if content_divs:
            # Ä°Ã§erik en uzun olan div'i bul
            max_div = max(content_divs, key=lambda x: x[1])
            # Sadece get_text() Ã§aÄŸÄ±rÄ±n, sonra temizleyin
            main_content_text = max_div[0].get_text()
        else:
            # Son Ã§are: TÃ¼m metni al (simplify_html zaten temizlenmiÅŸ HTML verdiÄŸini varsayÄ±yoruz)
            # Sadece get_text() Ã§aÄŸÄ±rÄ±n, sonra temizleyin
            main_content_text = soup.get_text()

    # Metni temizle: Fazla boÅŸluklarÄ± ve satÄ±r sonlarÄ±nÄ± tek boÅŸluÄŸa dÃ¶nÃ¼ÅŸtÃ¼r ve baÅŸtaki/sondaki boÅŸluklarÄ± kÄ±rp
    cleaned_text = re.sub(r'\s+', ' ', main_content_text).strip()

    return cleaned_text

def simplify_html(html_content):
    """
    HTML'i sadeleÅŸtir, sadece <article> iÃ§eriÄŸini (veya fallback olarak <main>/<body>) al,
    ve gereksiz kÄ±sÄ±mlarÄ± kaldÄ±r.
"""
    soup = BeautifulSoup(html_content, 'html.parser')
    target_element = None

    # Ã–nce <article> etiketini ara
    article_element = soup.find('article')
    if article_element:
        target_element = article_element
    else:
        # <article> yoksa <main> etiketini ara
        main_element = soup.find('main')
        if main_element:
            target_element = main_element
        else:
            # <main> de yoksa <body> etiketini kullan (varsa)
            target_element = soup.body

    # EÄŸer hedef element bulunduysa (article, main veya body)
    if target_element:
        # Hedef element iÃ§indeki gereksiz etiketleri kaldÄ±r
        for tag in target_element.select('script, style, meta, link, iframe, img, nav, footer'):
            # header ve footer gibi genel sarmalayÄ±cÄ±larÄ± da kaldÄ±ralÄ±m
            tag.decompose()

        # TemizlenmiÅŸ hedef elementin string temsilini dÃ¶ndÃ¼r
        return str(target_element)
    else:
        # HiÃ§bir ana iÃ§erik elementi bulunamazsa (body dahil), tÃ¼m soup'un metnini dÃ¶ndÃ¼r
        # veya boÅŸ bir string dÃ¶ndÃ¼r - duruma gÃ¶re karar verilebilir.
        # Åimdilik temizlenmiÅŸ tÃ¼m soup'u dÃ¶ndÃ¼relim (script/style vb. kaldÄ±rÄ±lmÄ±ÅŸ haliyle)
        for tag in soup.select('script, style, meta, link, iframe, img, nav, footer'):
            tag.decompose()
        return str(soup)
